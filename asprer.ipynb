{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UWDzE9x_lWPa",
        "YCOL-IVBl6UD",
        "g3KKbD72l-7X",
        "iDV1FBaWmMwi",
        "Mmz-yaCEmO3L",
        "tTunkk4umQ7N",
        "7Y4Ib0hznO1R",
        "Cz9KdO-yrn_H",
        "-s9e8Z11nTqp",
        "FKoYA4X5lsCO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Live Demo"
      ],
      "metadata": {
        "id": "vElq-RGpucK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = nlp(input(\"Enter sentence: \"))\n",
        "fill_in_the_blank_questions = []\n",
        "who_what_where_questions = []\n",
        "true_true_or_false_questions= []\n",
        "false_true_or_false_questions = []\n",
        "multiple_choice_questions = MultipleChoice(\"Curie\", 10, 0.20, 0.80).create()\n",
        "\n",
        "for sentence in input_sentence.sents:\n",
        "    fill_in_the_blank_questions.append(FillInTheBlank(sentence, matcher, fill_in_the_blank_pattern).create())\n",
        "    who_what_where_questions.append(WhoWhatWhere(sentence, matcher, who_what_where_pattern).create())\n",
        "    true_true_or_false_questions.append(TrueOrFalse(sentence, matcher, true_or_false_pattern).create())\n",
        "\n",
        "try:\n",
        "    for true_or_false_set in true_true_or_false_questions:\n",
        "        for true_or_false in true_or_false_set:\n",
        "            if true_or_false != None:\n",
        "                false_true_or_false_questions.append(TrueOrFalseDistractor(true_or_false, 10, 0.20, 0.80).create())\n",
        "except TypeError:\n",
        "    false_true_or_false_questions.append(None)\n",
        "\n",
        "print(f\"Fill In The Blank Questions: {fill_in_the_blank_questions}\")\n",
        "print(f\"Who-What-Where Questions: {who_what_where_questions}\")\n",
        "print(f\"(True) True or False Questions: {true_true_or_false_questions}\")\n",
        "print(f\"(False) True or False Questions: {false_true_or_false_questions}\")\n",
        "print(f\"Multiple Choice Questions: {multiple_choice_questions}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk4uBJUAurpx",
        "outputId": "f3ac13cb-03ab-4f38-f443-7e8c669fad25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter sentence: ATP is the currency of the cell.\n",
            "Fill In The Blank Questions: [[['ATP __ the currency of the cell .', ['is']], ['___ is the currency of the cell .', ['ATP']], ['ATP is the currency of the ____ .', ['cell']], ['ATP is the ________ of the cell .', ['currency']], ['___ __ the currency of the cell .', ['is', 'ATP']], ['ATP __ the currency of the ____ .', ['is', 'cell']], ['ATP __ the ________ of the cell .', ['is', 'currency']], ['___ is the currency of the ____ .', ['ATP', 'cell']], ['___ is the ________ of the cell .', ['ATP', 'currency']], ['ATP is the ________ of the ____ .', ['cell', 'currency']], ['___ __ the currency of the ____ .', ['is', 'ATP', 'cell']], ['___ __ the ________ of the cell .', ['is', 'ATP', 'currency']], ['ATP __ the ________ of the ____ .', ['is', 'cell', 'currency']], ['___ is the ________ of the ____ .', ['ATP', 'cell', 'currency']], ['___ __ the ________ of the ____ .', ['is', 'ATP', 'cell', 'currency']]]]\n",
            "Who-What-Where Questions: [None]\n",
            "(True) True or False Questions: [['Is ATP the currency of the cell?']]\n",
            "(False) True or False Questions: [['Is Federer the currency of the cell?', 'Is Roddick the currency of the cell?', 'Is Nadal the currency of the cell?', 'Is Djokovic the currency of the cell?', 'Is Pga the currency of the cell?', 'Is Garros the currency of the cell?', 'Is Rafael the currency of the cell?', 'Is Wimbledon the currency of the cell?']]\n",
            "Multiple Choice Questions: [['Becquerel', 'Curies', 'Galilei'], ['Becquerel', 'Curies', 'Lavoisier'], ['Becquerel', 'Curies', 'Mendeleev'], ['Becquerel', 'Curies', 'Planck'], ['Becquerel', 'Curies', 'Radium'], ['Becquerel', 'Curies', 'Bohr'], ['Becquerel', 'Curies', 'Marie'], ['Becquerel', 'Galilei', 'Lavoisier'], ['Becquerel', 'Galilei', 'Mendeleev'], ['Becquerel', 'Galilei', 'Planck'], ['Becquerel', 'Galilei', 'Radium'], ['Becquerel', 'Galilei', 'Bohr'], ['Becquerel', 'Galilei', 'Marie'], ['Becquerel', 'Lavoisier', 'Mendeleev'], ['Becquerel', 'Lavoisier', 'Planck'], ['Becquerel', 'Lavoisier', 'Radium'], ['Becquerel', 'Lavoisier', 'Bohr'], ['Becquerel', 'Lavoisier', 'Marie'], ['Becquerel', 'Mendeleev', 'Planck'], ['Becquerel', 'Mendeleev', 'Radium'], ['Becquerel', 'Mendeleev', 'Bohr'], ['Becquerel', 'Mendeleev', 'Marie'], ['Becquerel', 'Planck', 'Radium'], ['Becquerel', 'Planck', 'Bohr'], ['Becquerel', 'Planck', 'Marie'], ['Becquerel', 'Radium', 'Bohr'], ['Becquerel', 'Radium', 'Marie'], ['Becquerel', 'Bohr', 'Marie'], ['Curies', 'Galilei', 'Lavoisier'], ['Curies', 'Galilei', 'Mendeleev'], ['Curies', 'Galilei', 'Planck'], ['Curies', 'Galilei', 'Radium'], ['Curies', 'Galilei', 'Bohr'], ['Curies', 'Galilei', 'Marie'], ['Curies', 'Lavoisier', 'Mendeleev'], ['Curies', 'Lavoisier', 'Planck'], ['Curies', 'Lavoisier', 'Radium'], ['Curies', 'Lavoisier', 'Bohr'], ['Curies', 'Lavoisier', 'Marie'], ['Curies', 'Mendeleev', 'Planck'], ['Curies', 'Mendeleev', 'Radium'], ['Curies', 'Mendeleev', 'Bohr'], ['Curies', 'Mendeleev', 'Marie'], ['Curies', 'Planck', 'Radium'], ['Curies', 'Planck', 'Bohr'], ['Curies', 'Planck', 'Marie'], ['Curies', 'Radium', 'Bohr'], ['Curies', 'Radium', 'Marie'], ['Curies', 'Bohr', 'Marie'], ['Galilei', 'Lavoisier', 'Mendeleev'], ['Galilei', 'Lavoisier', 'Planck'], ['Galilei', 'Lavoisier', 'Radium'], ['Galilei', 'Lavoisier', 'Bohr'], ['Galilei', 'Lavoisier', 'Marie'], ['Galilei', 'Mendeleev', 'Planck'], ['Galilei', 'Mendeleev', 'Radium'], ['Galilei', 'Mendeleev', 'Bohr'], ['Galilei', 'Mendeleev', 'Marie'], ['Galilei', 'Planck', 'Radium'], ['Galilei', 'Planck', 'Bohr'], ['Galilei', 'Planck', 'Marie'], ['Galilei', 'Radium', 'Bohr'], ['Galilei', 'Radium', 'Marie'], ['Galilei', 'Bohr', 'Marie'], ['Lavoisier', 'Mendeleev', 'Planck'], ['Lavoisier', 'Mendeleev', 'Radium'], ['Lavoisier', 'Mendeleev', 'Bohr'], ['Lavoisier', 'Mendeleev', 'Marie'], ['Lavoisier', 'Planck', 'Radium'], ['Lavoisier', 'Planck', 'Bohr'], ['Lavoisier', 'Planck', 'Marie'], ['Lavoisier', 'Radium', 'Bohr'], ['Lavoisier', 'Radium', 'Marie'], ['Lavoisier', 'Bohr', 'Marie'], ['Mendeleev', 'Planck', 'Radium'], ['Mendeleev', 'Planck', 'Bohr'], ['Mendeleev', 'Planck', 'Marie'], ['Mendeleev', 'Radium', 'Bohr'], ['Mendeleev', 'Radium', 'Marie'], ['Mendeleev', 'Bohr', 'Marie'], ['Planck', 'Radium', 'Bohr'], ['Planck', 'Radium', 'Marie'], ['Planck', 'Bohr', 'Marie'], ['Radium', 'Bohr', 'Marie']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "UWDzE9x_lWPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "import random\n",
        "import time"
      ],
      "metadata": {
        "id": "R2RDy9ezlXUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Patterns"
      ],
      "metadata": {
        "id": "YCOL-IVBl6UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fill_in_the_blank_pattern = [[\n",
        "    {\"POS\": \"PROPN\", \"OP\": \"?\"    },\n",
        "    {\"POS\": \"NOUN\", \"OP\": \"?\"     },\n",
        "    {\"POS\": \"ADJECTIVE\", \"OP\": \"?\"},\n",
        "    {\"POS\": \"VERB\", \"OP\": \"?\"     },\n",
        "    {\"ENT_TYPE\": \"CARDINAL\", \"OP\": \"?\"},\n",
        "    {\"ENT_TYPE\": \"DATE\", \"OP\": \"*\"},\n",
        "    {\"ENT_TYPE\": \"EVENT\", \"OP\": \"?\"},\n",
        "    {\"ENT_TYPE\": \"WORK_OF_ART\", \"OP\": \"?\"},\n",
        "    {\"ENT_TYPE\": \"NORP\", \"OP\": \"?\"},\n",
        "    {\"ENT_TYPE\": \"GPE\", \"OP\": \"?\"},\n",
        "]]\n",
        "\n",
        "true_or_false_pattern = [[\n",
        "    {\"POS\": \"VERB\", \"OP\": \"?\"},\n",
        "    {\"POS\": \"AUX\", \"OP\": \"?\"},\n",
        "]]\n",
        "\n",
        "who_what_where_pattern = [[\n",
        "    {\"POS\": \"VERB\", \"OP\": \"?\"},\n",
        "    {\"POS\": \"AUX\", \"OP\": \"?\"},\n",
        "]]"
      ],
      "metadata": {
        "id": "QSzOQGIklpuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spaCy"
      ],
      "metadata": {
        "id": "g3KKbD72l-7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "nlp_sm = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)"
      ],
      "metadata": {
        "id": "P9dIlktYmAT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fill In The Blanks"
      ],
      "metadata": {
        "id": "iDV1FBaWmMwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FillInTheBlank():\n",
        "    \"\"\" A class to generate fill-in-the-blanks question \"\"\"\n",
        "\n",
        "    def __init__(self, sentence, matcher, fill_in_the_blank_pattern):\n",
        "        self.sentence = sentence\n",
        "        self.matcher = matcher\n",
        "        self.fill_in_the_blank_pattern = fill_in_the_blank_pattern\n",
        "        self.matcher.add(\"get_to_blanks\", self.fill_in_the_blank_pattern)\n",
        "\n",
        "    \"\"\" Get all `to_blanks` token matched by the `fill_in_the_blank_pattern` \"\"\"\n",
        "\n",
        "    def _get_to_blanks(self):\n",
        "        to_blanks_match = self.matcher(self.sentence)\n",
        "        to_blanks = [self.sentence[start].text for _, start, end in to_blanks_match]\n",
        "\n",
        "        return list(set(to_blanks))\n",
        "\n",
        "    \"\"\" Get all possible `to_blanks` combination \"\"\"\n",
        "\n",
        "    def _get_to_blanks_combinations(self, to_blanks):\n",
        "        to_blanks_combinations = []\n",
        "\n",
        "        for i in range(1, len(to_blanks) + 1):\n",
        "            for combo in combinations(to_blanks, i):\n",
        "                to_blanks_combinations.append(combo)\n",
        "        return to_blanks_combinations\n",
        "\n",
        "    \"\"\" Structurize the question \"\"\"\n",
        "\n",
        "    def _get_blanked_sentence(self, tokens):\n",
        "        blanked_sentence = []\n",
        "\n",
        "        for sentence_token in self.sentence:\n",
        "            to_append = []\n",
        "            if sentence_token.text in [token for token in tokens]:\n",
        "                to_append.append(len(sentence_token.text) * \"_\")\n",
        "            else:\n",
        "                to_append.append(sentence_token.text)\n",
        "            blanked_sentence.append(*to_append)\n",
        "\n",
        "        return \" \".join(blanked_sentence)\n",
        "\n",
        "    \"\"\" A one call function to generate fill-in-the-blank question \"\"\"\n",
        "\n",
        "    def create(self):\n",
        "        to_blanks = self._get_to_blanks()\n",
        "        combinations = self._get_to_blanks_combinations(to_blanks)\n",
        "\n",
        "        fill_in_the_blank = []\n",
        "\n",
        "        for combo in combinations:\n",
        "            blanked_sentence = [self._get_blanked_sentence((combo))]\n",
        "            blanked_sentence.append(list(combo))\n",
        "            fill_in_the_blank.append(blanked_sentence)\n",
        "\n",
        "        self.matcher.remove(\"get_to_blanks\")\n",
        "\n",
        "        return fill_in_the_blank"
      ],
      "metadata": {
        "id": "nKAmEnlfpxRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple Choice"
      ],
      "metadata": {
        "id": "Mmz-yaCEmO3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultipleChoice():\n",
        "\n",
        "    \"\"\" A class to generate a multiple-choice question based on a 'who what where' question \"\"\"\n",
        "\n",
        "    def __init__(self, right_answer, similarity_token_size, minimum_similarity, maximum_similarity):\n",
        "        self.right_answer = right_answer\n",
        "        self.similarity_token_size = similarity_token_size\n",
        "        self.minimum_similarity = minimum_similarity\n",
        "        self.maximum_similarity = maximum_similarity\n",
        "\n",
        "    \"\"\" Get the similar token from the subject of the sentence \"\"\"\n",
        "\n",
        "    def _get_distractors(self):\n",
        "        try:\n",
        "\n",
        "            most_similar_answers = nlp.vocab.vectors.most_similar(\n",
        "                np.asarray([nlp.vocab.vectors[nlp.vocab.strings[self.right_answer]]]),\n",
        "                n=self.similarity_token_size\n",
        "            )\n",
        "\n",
        "            similar_answers = [(nlp.vocab.strings[w]).capitalize() for w in most_similar_answers[0][0]]\n",
        "\n",
        "            distances = most_similar_answers[2][0]\n",
        "\n",
        "            distractors = []\n",
        "\n",
        "            for index, distance in enumerate(distances):\n",
        "                if distance >= self.minimum_similarity and distance <= self.maximum_similarity:\n",
        "                    distractors.append(similar_answers[index])\n",
        "\n",
        "            return distractors\n",
        "        except (KeyError, AttributeError):\n",
        "            return None\n",
        "    \"\"\" Get the combination of all distractors \"\"\"\n",
        "\n",
        "    def _get_distractors_combination(self, distractors):\n",
        "        return [list(combo) for combo in combinations(distractors, 3)]\n",
        "\n",
        "    \"\"\" index 0 is always the right answer \"\"\"\n",
        "\n",
        "    def create(self):\n",
        "        distractors = self._get_distractors()\n",
        "        if distractors == None:\n",
        "            return None\n",
        "\n",
        "        distractors_combinations = self._get_distractors_combination(distractors)\n",
        "\n",
        "        return distractors_combinations"
      ],
      "metadata": {
        "id": "lP0FFfjEpzPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Who What Where"
      ],
      "metadata": {
        "id": "tTunkk4umQ7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WhoWhatWhere():\n",
        "\n",
        "    \"\"\" A class to generate who-what-where question \"\"\"\n",
        "\n",
        "    def __init__(self, sentence, matcher, who_what_where_pattern):\n",
        "        self.sentence = sentence\n",
        "        self.matcher = matcher\n",
        "        self.who_what_where_pattern = who_what_where_pattern\n",
        "\n",
        "    \"\"\" Get the `root_verb` of the input sentence \"\"\"\n",
        "\n",
        "    def _get_verbs(self):\n",
        "        self.matcher.add(\"get_verbs\", self.who_what_where_pattern)\n",
        "\n",
        "        verbs_match = self.matcher(self.sentence)\n",
        "\n",
        "        self.matcher.remove(\"get_verbs\")\n",
        "\n",
        "        verbs = [self.sentence[start] for _, start, end in verbs_match]\n",
        "        return verbs\n",
        "\n",
        "    def _get_root_verb(self, verbs):\n",
        "        root_verb = None\n",
        "\n",
        "        for verb in verbs:\n",
        "            if verb.dep_ == \"ROOT\":\n",
        "                root_verb = verb\n",
        "\n",
        "        return root_verb\n",
        "\n",
        "    def _get_root_verb_subjects(self, root_verb):\n",
        "        root_verb_subjects = []\n",
        "\n",
        "        for root_verb_children in root_verb.children:\n",
        "            if root_verb_children.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
        "                root_verb_subjects.append(root_verb_children)\n",
        "\n",
        "        return root_verb_subjects\n",
        "\n",
        "    def _get_root_verb_objects(self, root_verb):\n",
        "        root_verb_objects = []\n",
        "\n",
        "        for root_verb_children in root_verb.children:\n",
        "            if root_verb_children.dep_ in (\"dobj\", \"dobjpass\"):\n",
        "                root_verb_objects.append(root_verb_children)\n",
        "\n",
        "        return root_verb_objects\n",
        "\n",
        "\n",
        "    def _get_w_question_subjects(self, root_verb_subjects):\n",
        "        for root_verb_subject in root_verb_subjects:\n",
        "            if root_verb_subject.ent_type_ in (\"PERSON\", \"PRP\") or root_verb_subject.tag_ in (\"NN\"):\n",
        "                return \"who\"\n",
        "            elif root_verb_subject.ent_type_ == \"LOC\":\n",
        "                return \"where\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _get_w_question_objects(self, root_verb):\n",
        "        if root_verb.tag_ in (\"VBD\", \"VBN\"):\n",
        "            return \"what did\"\n",
        "        elif root_verb.tag_ in (\"VBP\", \"VBZ\"):\n",
        "            return \"what does\"\n",
        "        print(root_verb.tag_)\n",
        "        return None\n",
        "\n",
        "    def _get_remaining_text_subjects(self, root_verb_subjects):\n",
        "        remaining_text = []\n",
        "\n",
        "        for sentence_token in self.sentence:\n",
        "            if sentence_token not in root_verb_subjects:\n",
        "                remaining_text.append(sentence_token)\n",
        "\n",
        "        return remaining_text\n",
        "\n",
        "    def _get_remaining_text_objects(self, root_verb_objects):\n",
        "        remaining_text = []\n",
        "\n",
        "        for sentence_token in self.sentence:\n",
        "            if sentence_token not in root_verb_objects:\n",
        "                remaining_text.append(sentence_token)\n",
        "\n",
        "        return remaining_text\n",
        "\n",
        "    def _get_cleaned_remaining_text_subjects(self, remaining_text):\n",
        "        cleaned_remaining_text = []\n",
        "\n",
        "        for index, token in enumerate(remaining_text):\n",
        "            if index == 0:\n",
        "                if token.tag_ != \"DT\":\n",
        "                    cleaned_remaining_text.append(token)\n",
        "            else:\n",
        "                    cleaned_remaining_text.append(token)\n",
        "\n",
        "        cleaned = [token.text for token in cleaned_remaining_text if token.tag_ != \"VBN\" or token.dep_ == \"ROOT\"]\n",
        "\n",
        "        return \" \".join(cleaned)\n",
        "    def _get_cleaned_remaining_text_objects(self, remaining_text):\n",
        "        cleaned_remaining_text = []\n",
        "\n",
        "        for index, token in enumerate(remaining_text):\n",
        "            if index == len(remaining_text) - 1:\n",
        "                if token.tag_ != \"DT\":\n",
        "                    cleaned_remaining_text.append(token)\n",
        "            else:\n",
        "                    cleaned_remaining_text.append(token)\n",
        "\n",
        "        cleaned = [token.text for token in cleaned_remaining_text if token.tag_ != \"VBN\" or token.dep_ == \"ROOT\"]\n",
        "\n",
        "        return \" \".join(cleaned)\n",
        "\n",
        "    def create(self):\n",
        "        who_what_where_questions = []\n",
        "\n",
        "        verbs = self._get_verbs()\n",
        "        root_verb = self._get_root_verb(verbs)\n",
        "\n",
        "        # Who, Where\n",
        "        root_verb_subjects = self._get_root_verb_subjects(root_verb)\n",
        "\n",
        "        # What\n",
        "        root_verb_objects = self._get_root_verb_objects(root_verb)\n",
        "\n",
        "\n",
        "        if len(root_verb_subjects) > 0:\n",
        "            w_question_subjects = self._get_w_question_subjects(root_verb_subjects)\n",
        "\n",
        "            if w_question_subjects == None:\n",
        "                return None\n",
        "\n",
        "            remaining_text = self._get_remaining_text_subjects(root_verb_subjects)\n",
        "            cleaned_remaining_text_subjects = self._get_cleaned_remaining_text_subjects(remaining_text)\n",
        "\n",
        "            to_append = f\"{w_question_subjects.capitalize()} {cleaned_remaining_text_subjects}?\", [subjects.text for subjects in root_verb_subjects]\n",
        "            who_what_where_questions.append(to_append)\n",
        "\n",
        "        if len(root_verb_objects) > 0:\n",
        "            w_question_objects = self._get_w_question_objects(root_verb)\n",
        "\n",
        "            if w_question_objects == None:\n",
        "                return None\n",
        "\n",
        "            remaining_text = self._get_remaining_text_objects(root_verb_objects)\n",
        "            cleaned_remaining_text_objects = self._get_cleaned_remaining_text_objects(remaining_text)\n",
        "\n",
        "            to_append = f\"{w_question_objects.capitalize()} {cleaned_remaining_text_objects}?\", [objects.text for objects in root_verb_objects]\n",
        "            who_what_where_questions.append(to_append)\n",
        "\n",
        "        return who_what_where_questions"
      ],
      "metadata": {
        "id": "fBSbSflPrflF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# True or False"
      ],
      "metadata": {
        "id": "7Y4Ib0hznO1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrueOrFalse():\n",
        "\n",
        "    \"\"\" A class to generate true-or-false type of question \"\"\"\n",
        "\n",
        "    def __init__(self, sentence, matcher, true_or_false_pattern):\n",
        "        self.sentence = sentence\n",
        "        self.matcher = matcher\n",
        "        self.true_or_false_pattern = true_or_false_pattern\n",
        "        self.matcher.add(\"get_verbs\", self.true_or_false_pattern)\n",
        "\n",
        "    \"\"\" Get the verbs found in the input sentence \"\"\"\n",
        "\n",
        "    def _get_verbs(self):\n",
        "        verbs_match = self.matcher(self.sentence)\n",
        "\n",
        "        verbs = [self.sentence[start] for _, start, end in verbs_match]\n",
        "        return verbs\n",
        "\n",
        "    \"\"\" Get the root verb from the verbs \"\"\"\n",
        "\n",
        "    def _get_root_verb(self, verbs):\n",
        "        root_verb = None\n",
        "\n",
        "        for verb in verbs:\n",
        "            if verb.dep_ == \"ROOT\":\n",
        "                root_verb = verb\n",
        "\n",
        "        return root_verb\n",
        "\n",
        "    \"\"\" Get the helping verbs  \"\"\"\n",
        "\n",
        "    def _get_auxilliary_verbs(self, verbs):\n",
        "        auxilliary_verbs = []\n",
        "\n",
        "        for verb in verbs:\n",
        "            if verb.dep_ in (\"auxpass\", \"aux\"):\n",
        "                auxilliary_verbs.append(verb)\n",
        "\n",
        "        return auxilliary_verbs\n",
        "\n",
        "    \"\"\" Get the helping verbs  \"\"\"\n",
        "\n",
        "    def _get_main_verb(self, root_verb, auxilliary_verbs):\n",
        "\n",
        "        if root_verb == None:\n",
        "            return None\n",
        "\n",
        "        if len(root_verb) > 0:\n",
        "            return root_verb\n",
        "        else:\n",
        "            return auxilliary_verbs\n",
        "\n",
        "    \"\"\" Get the type of verb (helping or not) \"\"\"\n",
        "\n",
        "    def _get_main_verb_type(self, main_verb):\n",
        "        if main_verb.dep_ in (\"auxpass\", \"aux\"):\n",
        "            return \"AUX\"\n",
        "        else:\n",
        "            return main_verb.pos_\n",
        "\n",
        "    \"\"\" Verb fronting \"\"\"\n",
        "\n",
        "    def _get_verb_to_front(self, main_verb, main_verb_type):\n",
        "        if main_verb_type == \"AUX\":\n",
        "            return main_verb.text\n",
        "        elif main_verb_type in (\"VBD\", \"VBN\"):\n",
        "            return \"did\"\n",
        "        elif main_verb_type in (\"VBP\", \"VBZ\"):\n",
        "            return \"does\"\n",
        "        else:\n",
        "            return \"Cannot determine the verb to front...\"\n",
        "\n",
        "    \"\"\" Get the subjects of the main verb \"\"\"\n",
        "\n",
        "    def _get_main_verb_subjects(self, main_verb):\n",
        "        main_verb_subjects = []\n",
        "\n",
        "        for children in main_verb.children:\n",
        "            if children.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
        "                main_verb_subjects.append(children.text)\n",
        "\n",
        "        return main_verb_subjects\n",
        "\n",
        "    \"\"\" Get the remaining subject text \"\"\"\n",
        "\n",
        "    def _get_remaining_text(self, main_verb, main_verb_type, main_verb_subjects):\n",
        "        remaining_text = []\n",
        "\n",
        "        if main_verb_type == \"AUX\":\n",
        "            for sentence_token in self.sentence:\n",
        "                if sentence_token.text != main_verb.text:\n",
        "                    remaining_text.append(sentence_token)\n",
        "        elif main_verb_type in (\"VBD\", \"VBN\", \"VBP\", \"VBZ\"):\n",
        "            for sentence_token in self.sentence:\n",
        "                if sentence_token.text not in (main_verb.text, *main_verb_subjects):\n",
        "                    remaining_text.append(sentence_token)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        return remaining_text\n",
        "\n",
        "    \"\"\" Clean the remaining text \"\"\"\n",
        "\n",
        "    def _get_cleaned_remaining_text(self, remaining_text):\n",
        "        cleaned_remaining_text = []\n",
        "\n",
        "        if remaining_text == None:\n",
        "            return \"\"\n",
        "\n",
        "        for token in remaining_text:\n",
        "            if token.tag_ not in (\"-LRB-\", \"-RRB-\", \"``\", \"''\", \".\") and token.ent_type_ != \"DATE\" and token.dep_ != \"auxpass\":\n",
        "                cleaned_remaining_text.append(token.text)\n",
        "\n",
        "        return \" \".join(cleaned_remaining_text)\n",
        "\n",
        "    \"\"\" A one call function to create a true-or-false question \"\"\"\n",
        "\n",
        "    def create(self):\n",
        "        true_or_false_questions = []\n",
        "\n",
        "        verbs = self._get_verbs()\n",
        "        root_verb = self._get_root_verb(verbs)\n",
        "        auxilliary_verbs = self._get_auxilliary_verbs(verbs)\n",
        "\n",
        "        main_verb = self._get_main_verb(root_verb, auxilliary_verbs)\n",
        "\n",
        "        if main_verb == None:\n",
        "            return None\n",
        "\n",
        "        main_verb_type = self._get_main_verb_type(main_verb)\n",
        "\n",
        "        verb_to_front = self._get_verb_to_front(main_verb, main_verb_type)\n",
        "\n",
        "        main_verb_subjects = self._get_main_verb_subjects(main_verb)\n",
        "\n",
        "        remaining_text = self._get_remaining_text(main_verb, main_verb_type, main_verb_subjects)\n",
        "        cleaned_remaining_text = self._get_cleaned_remaining_text(remaining_text)\n",
        "\n",
        "        # Prioritize auxilliary verbs\n",
        "        if main_verb_type == \"AUX\":\n",
        "            true_or_false_question = f\"{verb_to_front.capitalize()} {cleaned_remaining_text}?\"\n",
        "            true_or_false_questions.append(true_or_false_question)\n",
        "        elif main_verb_type in (\"VBD\", \"VBN\", \"VBP\", \"VBZ\"):\n",
        "            true_or_false_question = f\"{verb_to_front.capitalize()} {' '.join(main_verb_subjects)} {root_verb.lemma_} {cleaned_remaining_text}?\"\n",
        "            true_or_false_questions.append(true_or_false_question)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        self.matcher.remove(\"get_verbs\")\n",
        "\n",
        "        return true_or_false_questions"
      ],
      "metadata": {
        "id": "GITOxK4Yp1f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrueOrFalseDistractor():\n",
        "\n",
        "    \"\"\" A class to generate a distractor \"\"\"\n",
        "\n",
        "    def __init__(self, true_or_false, similar_token_size, minimum_similarity, maximum_similarity):\n",
        "        self.true_or_false = true_or_false\n",
        "        self.similar_token_size = similar_token_size\n",
        "        self.minimum_similarity = minimum_similarity\n",
        "        self.maximum_similarity = maximum_similarity\n",
        "\n",
        "    \"\"\" Get the token to generate the distractor to \"\"\"\n",
        "\n",
        "    def _get_token_to_switch(self):\n",
        "        true_or_false = nlp(self.true_or_false)\n",
        "\n",
        "        for token in true_or_false:\n",
        "            if token.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
        "                return token\n",
        "        return \"\"\n",
        "\n",
        "    \"\"\" Get the similar tokens from the tokens to switch \"\"\"\n",
        "\n",
        "    def _get_most_similar_tokens_from_token(self, token):\n",
        "        try:\n",
        "            most_similar_tokens = nlp.vocab.vectors.most_similar(np.asarray([nlp.vocab.vectors[nlp.vocab.strings[token.text]]]), n=self.similar_token_size)\n",
        "\n",
        "            similar_tokens = [(nlp.vocab.strings[w]).capitalize() for w in most_similar_tokens[0][0]]\n",
        "\n",
        "            distances = most_similar_tokens[2][0]\n",
        "            tokens = []\n",
        "\n",
        "            for index, distance in enumerate(distances):\n",
        "                if distance >= self.minimum_similarity and distance <= self.maximum_similarity:\n",
        "                    tokens.append(similar_tokens[index])\n",
        "\n",
        "            return tokens\n",
        "        except (KeyError, AttributeError):\n",
        "            return None\n",
        "\n",
        "    \"\"\" A one call function to create a false type question of true or false \"\"\"\n",
        "\n",
        "    def create(self):\n",
        "        token_to_switch = self._get_token_to_switch()\n",
        "\n",
        "        if token_to_switch == None:\n",
        "            return None\n",
        "\n",
        "        similar_tokens = self._get_most_similar_tokens_from_token(token_to_switch)\n",
        "\n",
        "        if similar_tokens == None or similar_tokens == []:\n",
        "            return None\n",
        "\n",
        "        if self.true_or_false == None:\n",
        "            return None\n",
        "\n",
        "\n",
        "        true_or_false_sentence = self.true_or_false.split()\n",
        "        true_or_false_distractor = []\n",
        "\n",
        "        for distractor in similar_tokens:\n",
        "            current_distractor = []\n",
        "            for word in true_or_false_sentence:\n",
        "                if token_to_switch.text == word:\n",
        "                    current_distractor.append(distractor)\n",
        "                else:\n",
        "                    current_distractor.append(word)\n",
        "            distracted_true_or_false = \" \".join(current_distractor)\n",
        "            true_or_false_distractor.append(distracted_true_or_false)\n",
        "\n",
        "        return true_or_false_distractor[1:]"
      ],
      "metadata": {
        "id": "Z-5NMm4Sp6eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bloom's Taxonomy"
      ],
      "metadata": {
        "id": "Cz9KdO-yrn_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blooms_taxonomy = {\n",
        "    \"true_or_false\": [\"Is\"],\n",
        "    \"remembering\": [ \"Choose\", \"Define\", \"Find\", \"How\", \"Label\", \"List\", \"Match\", \"Name\", \"Omit\", \"Recall\", \"Relate\", \"Select\", \"Show\", \"Spell\", \"Tell\", \"What\", \"When\", \"Where\", \"Which\", \"Who\", \"Why\"],\n",
        "    \"understanding\": [\"Classify\", \"Compare\", \"Contrast\", \"Demonstrate\", \"Explain\", \"Extend\", \"Illustrate\", \"Infer\", \"Interpret\", \"Outline\", \"Relate\", \"Rephrase\", \"Show\", \"Summarize\", \"Translate\"],\n",
        "    \"applying\": [\"Apply\", \"Build\", \"Choose\", \"Construct\", \"Develop\", \"Experiment\", \"Identify\", \"Interview\", \"Use\", \"Model\", \"Organize\", \"Plan\", \"Select\", \"Solve\", \"Utilize\"],\n",
        "    \"analyzing\": [\"Analyze\", \"Assume\", \"Categorize\", \"Classify\", \"Compare\", \"Conclusion\", \"Contrast\", \"Discover\", \"Dissect\", \"Distinguish\", \"Divide\", \"Examine\", \"Function\", \"Inference\", \"Inspect\", \"List\", \"Motive\", \"Relationships\", \"Simplify\", \"Survey\", \"Take part in\", \"Test for\", \"Theme\"],\n",
        "    \"evaluating\": [\"Agree\", \"Appraise\", \"Assess\", \"Award\", \"Choose\", \"Compare\", \"Conclude\", \"Criteria\", \"Criticize\", \"Decide\", \"Deduct\", \"Defend\", \"Determine\", \"Disprove\", \"Estimate\", \"Evaluate\", \"Explain\", \"Importance\", \"Influence\", \"Interpret\", \"Judge\", \"Justify\", \"Mark\", \"Measure\", \"Opinion\", \"Perceive\", \"Prioritize\", \"Prove\", \"Rate\", \"Recommend\", \"Rule on\", \"Select\", \"Support\", \"Value\"],\n",
        "    \"creating\": [\"Adapt\", \"Build\", \"Change\", \"Choose\", \"Combine\", \"Compile\", \"Compose\", \"Construct\", \"Create\", \"Delete\", \"Design\", \"Develop\", \"Discuss\", \"Elaborate\", \"Estimate\", \"Formulate\", \"Happen\", \"Imagine\", \"Improve\", \"Invent\", \"Make up\", \"Maximize\", \"Minimize\", \"Modify\", \"Original\", \"Originate\", \"Plan\", \"Predict\", \"Propose\", \"Solution\", \"Solve\", \"Suppose\", \"Test\", \"Theory\"],\n",
        "}"
      ],
      "metadata": {
        "id": "KbQ62dCXrpIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assessment Generator"
      ],
      "metadata": {
        "id": "-s9e8Z11nTqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AssessmentGenerator():\n",
        "\n",
        "    \"\"\" A class to find the best set of assessment questions that aligns to Bloom's Taxonomy \"\"\"\n",
        "\n",
        "    def __init__(self, assessment_questions, matcher, verb_pattern, assessment_set_size, assessment_question_size, mutation_rate, iteration_size, fitness_threshold):\n",
        "        self.assessment_questions = assessment_questions\n",
        "        self.matcher = matcher\n",
        "        self.matcher.add(\"get_verbs_from_assessment_question\", verb_pattern)\n",
        "        self.verb_pattern = verb_pattern\n",
        "        # The number of an assessment\n",
        "        self.assessment_set_size = assessment_set_size\n",
        "        # The number of questions in an assessment\n",
        "        self.assessment_question_size = assessment_question_size\n",
        "        # The chance in percent that a mutation will happen\n",
        "        self.mutation_rate = mutation_rate\n",
        "        # The number of iteration to stop finding for a solution\n",
        "        self.iteration_size = iteration_size\n",
        "        # Keep the solution if it passes this threshold\n",
        "        self.fitness_threshold = fitness_threshold\n",
        "\n",
        "    # Initialization\n",
        "\n",
        "    def _get_random_assessment_sets(self):\n",
        "        random_assessment_sets = []\n",
        "\n",
        "        for i in range(self.assessment_set_size):\n",
        "            current_assessment_set = []\n",
        "\n",
        "            for j in range(self.assessment_question_size):\n",
        "                random_assessment_question = self.assessment_questions[np.random.randint(0, len(self.assessment_questions) - 1)]\n",
        "\n",
        "                current_assessment_set.append(random_assessment_question)\n",
        "\n",
        "            random_assessment_sets.append(current_assessment_set)\n",
        "\n",
        "        return random_assessment_sets\n",
        "\n",
        "    # Evaluation\n",
        "\n",
        "    def _get_verbs_from_assessment_question(self, assessment_question):\n",
        "        doc = nlp_sm(assessment_question)\n",
        "        verbs_match = self.matcher(doc)\n",
        "        verbs = [doc[start].text for _, start, end in verbs_match]\n",
        "        return verbs\n",
        "\n",
        "    def _get_verb_hierarchy(self, verbs):\n",
        "        for key, taxonomy_verbs in blooms_taxonomy.items():\n",
        "            for taxonomy_verb in taxonomy_verbs:\n",
        "                if taxonomy_verb in verbs:\n",
        "                    return key\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _get_assessment_sets_hierarchy(self, assessment_sets):\n",
        "        assessment_sets_hierarchy = []\n",
        "\n",
        "        for assessment_set in assessment_sets:\n",
        "            current_assessment_set_hierarchy = []\n",
        "\n",
        "            for assessment_question in assessment_set:\n",
        "                current_assessment_question_hierarchy = {}\n",
        "                current_assessment_question_verb = self._get_verbs_from_assessment_question(assessment_question)\n",
        "                current_assessment_question_hierarchy[self._get_verb_hierarchy(current_assessment_question_verb)] = assessment_question\n",
        "                current_assessment_set_hierarchy.append(current_assessment_question_hierarchy)\n",
        "            assessment_sets_hierarchy.append(current_assessment_set_hierarchy)\n",
        "\n",
        "        return assessment_sets_hierarchy\n",
        "\n",
        "    # Make this configurable to the GUI (because much better to configure than manually writing codes.)\n",
        "    def _get_assessment_sets_quality(self, assessment_questions_hierarchy):\n",
        "        assessment_sets_quality = []\n",
        "\n",
        "        for assessment_questions_set in assessment_questions_hierarchy:\n",
        "            current_assessment_set_hierarchy = []\n",
        "\n",
        "            for assessment_question_set in assessment_questions_set:\n",
        "                for assessment_hierarchy, assessment_question in assessment_question_set.items():\n",
        "                    current_assessment_set_hierarchy.append(assessment_hierarchy)\n",
        "\n",
        "            current_assessment_set_hierarchy = set(current_assessment_set_hierarchy)\n",
        "\n",
        "\n",
        "            if len(current_assessment_set_hierarchy) == 1:\n",
        "                # Bad\n",
        "                if set(current_assessment_set_hierarchy) == {\"true_or_false\"}:\n",
        "                    assessment_sets_quality.append(0.25)\n",
        "                elif set(current_assessment_set_hierarchy) == {None}:\n",
        "                    assessment_sets_quality.append(0.35)\n",
        "                else:\n",
        "                    assessment_sets_quality.append(0.2)\n",
        "\n",
        "            elif len(current_assessment_set_hierarchy) == 2:\n",
        "                if set(current_assessment_set_hierarchy) == {\"analyzing\", \"evaluating\"}:\n",
        "                    # Medium\n",
        "                    assessment_sets_quality.append(0.4)\n",
        "\n",
        "                # Trivia specific\n",
        "                elif set(current_assessment_set_hierarchy) == {None, \"true_or_false\"}:\n",
        "                    assessment_sets_quality.append(0.55)\n",
        "                else:\n",
        "                    # Bad\n",
        "                    assessment_sets_quality.append(0.3)\n",
        "\n",
        "            elif len(current_assessment_set_hierarchy) == 3:\n",
        "                if set(current_assessment_set_hierarchy) == {\"understanding\", \"analyzing\", \"evaluating\"}:\n",
        "                    # Good\n",
        "                    assessment_sets_quality.append(0.7)\n",
        "                elif set(current_assessment_set_hierarchy) == {None, \"true_or_false\", \"remembering\"}:\n",
        "                    assessment_sets_quality.append(0.8)\n",
        "                else:\n",
        "                    # Medium\n",
        "                    assessment_sets_quality.append(0.5)\n",
        "\n",
        "            elif len(current_assessment_set_hierarchy) == 4:\n",
        "                if set(current_assessment_set_hierarchy) == {\"remembering\", \"understanding\", \"analyzing\", \"creating\"}:\n",
        "                    # Good\n",
        "                    assessment_sets_quality.append(0.8)\n",
        "                else:\n",
        "                    # Medium\n",
        "                    assessment_sets_quality.append(0.6)\n",
        "\n",
        "            elif len(current_assessment_set_hierarchy) == 5:\n",
        "                # Good\n",
        "                assessment_sets_quality.append(0.9)\n",
        "\n",
        "            elif len(current_assessment_set_hierarchy) == 6:\n",
        "                # Excellent\n",
        "                assessment_sets_quality.append(1.0)\n",
        "\n",
        "        return assessment_sets_quality\n",
        "\n",
        "    def _get_assessment_sets_fitness(self, assessment_sets_quality):\n",
        "        assessment_sets_fitness = []\n",
        "        for assessment_set_quality in assessment_sets_quality:\n",
        "            assessment_sets_fitness.append(1/assessment_set_quality)\n",
        "\n",
        "            # History\n",
        "            history.append(1/assessment_set_quality)\n",
        "        return assessment_sets_fitness\n",
        "\n",
        "    # Selection\n",
        "    def _get_assessment_sets_fitness_sum(self, assessment_sets_fitness):\n",
        "        assessment_sets_fitness_sum = 0\n",
        "        for assessment_set_fitness in assessment_sets_fitness:\n",
        "            assessment_sets_fitness_sum += assessment_set_fitness\n",
        "        return assessment_sets_fitness_sum\n",
        "\n",
        "    def _get_assessment_sets_fitness_probabilities(self, assessment_sets_fitness_sum, assessment_sets_fitness):\n",
        "        assessment_sets_fitness_probabilities = []\n",
        "        previous_probability = 0\n",
        "\n",
        "        for assessment_set_fitness in sorted(assessment_sets_fitness):\n",
        "            assessment_sets_fitness_probabilities.append(previous_probability + assessment_set_fitness / assessment_sets_fitness_sum)\n",
        "            previous_probability = previous_probability + assessment_set_fitness / assessment_sets_fitness_sum\n",
        "\n",
        "        return assessment_sets_fitness_probabilities\n",
        "\n",
        "    def _get_assessment_set_roulette(self, assessment_sets_fitness):\n",
        "        assessment_sets_fitness_sum = self._get_assessment_sets_fitness_sum(assessment_sets_fitness)\n",
        "        assessment_sets_fitness_probabilities = self._get_assessment_sets_fitness_probabilities(assessment_sets_fitness_sum, assessment_sets_fitness)\n",
        "\n",
        "        wheel_result = np.random.rand()\n",
        "\n",
        "        winner_index = 0\n",
        "\n",
        "        for index, assessment_set_fitness_probabilities in enumerate(assessment_sets_fitness_probabilities):\n",
        "            if wheel_result < assessment_set_fitness_probabilities:\n",
        "                winner_index = index\n",
        "                break\n",
        "\n",
        "        return winner_index\n",
        "\n",
        "    # Crossover\n",
        "    def _get_assessment_sets_crossover(self, assessment_set_roulette_1, assessment_set_roulette_2):\n",
        "        random_median = np.random.randint(1, len(assessment_set_roulette_1))\n",
        "        if np.random.randint(0, 1) == 0:\n",
        "            assessment_questions_1 = assessment_set_roulette_1[:random_median]\n",
        "            assessment_questions_2 = assessment_set_roulette_2[random_median:]\n",
        "            return assessment_questions_1 + assessment_questions_2\n",
        "        else:\n",
        "            assessment_questions_1 = assessment_set_roulette_2[:random_median]\n",
        "            assessment_questions_2 = assessment_set_roulette_1[random_median:]\n",
        "            return assessment_questions_1 + assessment_questions_2\n",
        "    # Mutation\n",
        "    def _get_assessment_sets_mutation(self, assessment_sets_crossover):\n",
        "        mutation = np.random.randint(1, 101)\n",
        "\n",
        "        if mutation <= self.mutation_rate:\n",
        "            random_assessment_position = np.random.randint(0, len(assessment_sets_crossover) - 1)\n",
        "            random_assessment_question = np.random.randint(0, len(self.assessment_questions) - 1)\n",
        "            assessment_sets_crossover[random_assessment_position] = self.assessment_questions[random_assessment_question]\n",
        "            return assessment_sets_crossover\n",
        "\n",
        "        return assessment_sets_crossover\n",
        "\n",
        "    def create(self):\n",
        "        solutions = self._get_random_assessment_sets()\n",
        "\n",
        "        solution_sets = []\n",
        "\n",
        "        for solution in range(self.iteration_size):\n",
        "            assessment_sets_hierarchy = self._get_assessment_sets_hierarchy(solutions)\n",
        "            assessment_sets_quality = self._get_assessment_sets_quality(assessment_sets_hierarchy)\n",
        "            assessment_sets_fitness = self._get_assessment_sets_fitness(assessment_sets_quality)\n",
        "\n",
        "            new_solutions = []\n",
        "\n",
        "            print(f\"Solution {solution}\")\n",
        "\n",
        "            for new_solution in range(self.assessment_set_size):\n",
        "                roulette_1_winner = self._get_assessment_set_roulette(assessment_sets_fitness)\n",
        "                roulette_2_winner = self._get_assessment_set_roulette(assessment_sets_fitness)\n",
        "\n",
        "                roulette_attempts = 0\n",
        "                while roulette_1_winner == roulette_2_winner and roulette_attempts < 100:\n",
        "                    roulette_2_winner = self._get_assessment_set_roulette(assessment_sets_fitness)\n",
        "                    roulette_attempts += 1\n",
        "\n",
        "                if roulette_1_winner == roulette_2_winner:\n",
        "                    assessment_set_roulette_1 = solutions[roulette_1_winner]\n",
        "                    assessment_set_roulette_2 = solutions[roulette_2_winner]\n",
        "                else:\n",
        "                    assessment_set_roulette_1 = solutions[roulette_1_winner]\n",
        "                    assessment_set_roulette_2 = solutions[roulette_2_winner]\n",
        "\n",
        "                # Crossover\n",
        "                assessment_sets_crossover = self._get_assessment_sets_crossover(assessment_set_roulette_1, assessment_set_roulette_2)\n",
        "                # Mutation\n",
        "                assessment_sets_mutation = self._get_assessment_sets_mutation(assessment_sets_crossover)\n",
        "\n",
        "                new_solutions.append(assessment_sets_mutation)\n",
        "\n",
        "\n",
        "            for index, solution in enumerate(solutions):\n",
        "                current_fitness = assessment_sets_fitness[index]\n",
        "                if current_fitness <= self.fitness_threshold:\n",
        "                    solution_sets.append(solution)\n",
        "\n",
        "            solutions = new_solutions\n",
        "\n",
        "        self.matcher.remove(\"get_verbs_from_assessment_question\")\n",
        "\n",
        "        for solution in solution_sets:\n",
        "            if len(set(solution)) < self.assessment_question_size:\n",
        "                solution_sets.remove(solution)\n",
        "\n",
        "        return solution_sets"
      ],
      "metadata": {
        "id": "zHjXULyOrr8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "FKoYA4X5lsCO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdT9zHd-lFAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c6c27a5-2539-4ab2-8ec3-918fa4a31eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python3 -m spacy download en_core_web_sm\n",
        "!python3 -m spacy download en_core_web_lg"
      ]
    }
  ]
}